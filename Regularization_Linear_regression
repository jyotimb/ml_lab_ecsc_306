clear ; close all; clc

fprintf('Loading and Visualizing Data ...\n')

load ('ex5data1.mat');

m = size(X, 1);
 
plot(X, y, 'rx', 'MarkerSize', 10, 'LineWidth', 1.5);
xlabel('Change in water level (x)');
ylabel('Water flowing out of the dam (y)');
 
fprintf('Program paused. Press enter to continue.\n');
pause;

 
theta = [1 ; 1];
J = linearRegCostFunction([ones(m, 1) X], y, theta, 1);
 
fprintf(['Cost at theta = [1 ; 1]: %f '...
         '\n(this value should be about 303.993192)\n'], J);
 
fprintf('Program paused. Press enter to continue.\n');
pause;

 
theta = [1 ; 1];
[J, grad] = linearRegCostFunction([ones(m, 1) X], y, theta, 1);
 
fprintf(['Gradient at theta = [1 ; 1]:  [%f; %f] '...
         '\n(this value should be about [-15.303016; 598.250744])\n'], ...
         grad(1), grad(2));
 
fprintf('Program paused. Press enter to continue.\n');
pause;
 
lambda = 0;
[theta] = trainLinearReg([ones(m, 1) X], y, lambda);
 
%  Plot fit over the data
plot(X, y, 'rx', 'MarkerSize', 10, 'LineWidth', 1.5);
xlabel('Change in water level (x)');
ylabel('Water flowing out of the dam (y)');
hold on;
plot(X, [ones(m, 1) X]*theta, '--', 'LineWidth', 2)
hold off;
 
fprintf('Program paused. Press enter to continue.\n');
pause;

 
lambda = 0;
[error_train, error_val] = ...
    learningCurve([ones(m, 1) X], y, ...
                  [ones(size(Xval, 1), 1) Xval], yval, ...
                  lambda);
 
plot(1:m, error_train, 1:m, 

plot(1:m, error_train, 1:m, error_val);
title('Learning curve for linear regression')
legend('Train', 'Cross Validation')
xlabel('Number of training examples')
ylabel('Error')
axis([0 13 0 150])
 
fprintf('# Training Examples\tTrain Error\tCross Validation Error\n');
for i = 1:m
    fprintf('  \t%d\t\t%f\t%f\n', i, error_train(i), error_val(i));
end
 
fprintf('Program paused. Press enter to continue.\n');
pause;

 
% Map X onto Polynomial Features and Normalize
X_poly = polyFeatures(X, p);
[X_poly, mu, sigma] = featureNormalize(X_poly);  % Normalize
X_poly = [ones(m, 1), X_poly];                   % Add Ones
 
% Map X_poly_test and normalize (using mu and sigma)
X_poly_test = polyFeatures(Xtest, p);
X_poly_test = bsxfun(@minus, X_poly_test, mu);
X_poly_test = bsxfun(@rdivide, X_poly_test, sigma);
X_poly_test = [ones(size(X_poly_test, 1), 1), X_poly_test];         % Add Ones
 
% Map X_poly_val and normalize (using mu and sigma)
X_poly_val = polyFeatures(Xval, p);
X_poly_val = bsxfun(@minus, X_poly_val, mu);
X_poly_val = bsxfun(@rdivide, X_poly_val, sigma);
X_poly_val = [ones(size(X_poly_val, 1), 1), X_poly_val];           % Add Ones
 
fprintf('Normalized Training Example 1:\n');
fprintf('  %f  \n', X_poly(1, :));
 
fprintf('\nProgram paused. Press enter to continue.\n');
pause;
 

 
lambda = 0;
[theta] = trainLinearReg(X_poly, y, lambda);
 
% Plot training data and fit
figure(1);
plot(X, y, 'rx', 'MarkerSize', 10, 'LineWidth', 1.5);
plotFit(min(X), max(X), mu, sigma, theta, p);
xlabel('Change in water level (x)');
ylabel('Water flowing out of the dam (y)');
title (sprintf('Polynomial Regression Fit (lambda = %f)', lambda));
 
figure(2);
[error_train, error_val] = ...
    learningCurve(X_poly, y, X_poly_val, yval, lambda);
plot(1:m, error_train, 1:m, error_val);
 
title(sprintf('Polynomial Regression Learning Curve (lambda = %f)', lambda));
xlabel('Number of training examples')
ylabel('Error')
axis([0 13 0 100])
legend('Train', 'Cross Validation')
 
fprintf('Polynomial Regression (lambda = %f)\n\n', lambda);
fprintf('# Training Examples\tTrain Error\tCross Validation Error\n');
for i = 1:m
    fprintf('  \t%d\t\t%f\t%f\n', i, error_train(i), error_val(i));
end
  
fprintf('Program paused. Press enter to continue.\n');
pause;

 
[lambda_vec, error_train, error_val] = ...
    validationCurve(X_poly, y, X_poly_val, yval);
 
close all;
plot(lambda_vec, error_train, lambda_vec, error_val);
legend('Train', 'Cross Validation');
xlabel('lambda');
ylabel('Error');
 
fprintf('lambda\t\tTrain Error\tValidation Error\n');
for i = 1:length(lambda_vec)
    fprintf(' %f\t%f\t%f\n', ...
            lambda_vec(i), error_train(i), error_val(i));
end
 
fprintf('Program paused. Press enter to continue.\n');
pause;




//linearRegCostFunction

function [J, grad] = linearRegCostFunction(X, y, theta, lambda)
m = length(y); % number of training examples
J = 0;
grad = zeros(size(theta));
J = (1/(2*m))*sum(power((X*theta - y),2))+ (lambda/(2*m)) * sum(power(theta(2:end),2));
G = (lambda/m) .* theta;
G(1) = 0; 
grad = ((1/m) .* X' * (X*theta - y)) + G;
grad = grad(:);
end


//trainLinearReg

function [theta] = trainLinearReg(X, y, lambda)
initial_theta = zeros(size(X, 2), 1); 
costFunction = @(t) linearRegCostFunction(X, y, t, lambda);
 options = optimset('MaxIter', 200, 'GradObj', 'on');
theta = fmincg(costFunction, initial_theta, options);
 end




//learningCurve
function [error_train, error_val] = ...
    learningCurve(X, y, Xval, yval, lambda)
m = size(X, 1);
 

error_train = zeros(m, 1);
error_val   = zeros(m, 1);

for i = 1:m
    X_sub = X(1:i, :);
    y_sub = y(1:i); 
 
    theta = trainLinearReg(X_sub, y_sub, lambda);
 
    error_train(i) = linearRegCostFunction(X_sub, y_sub, theta, 0);
    error_val(i) = linearRegCostFunction(Xval, yval, theta, 0);
end
 end



//polyFeatures
function [X_poly] = polyFeatures(X, p)
X_poly = zeros(numel(X), p);
m = size(X,1);
for i=1:m
 
    poly_feature = zeros(p, 1);
 
    for j=1:p
        poly_feature(j) =  X(i).^j;
    end
 
    X_poly(i, <img src="https://s0.wp.com/wp-content/mu-plugins/wpcom-smileys/twemoji/2/svg/1f642.svg" alt="" class="emoji" draggable="false"> = poly_feature;
end 
end


//plotFit

function plotFit(min_x, max_x, mu, sigma, theta, p)
%PLOTFIT Plots a learned polynomial regression fit over an existing figure.
%Also works with linear regression.
%   PLOTFIT(min_x, max_x, mu, sigma, theta, p) plots the learned polynomial
%   fit with power p and feature normalization (mu, sigma).
 
% Hold on to the current figure
hold on;
 
% We plot a range slightly bigger than the min and max values to get
% an idea of how the fit will vary outside the range of the data points
x = (min_x - 15: 0.05 : max_x + 25)';
 
% Map the X values
X_poly = polyFeatures(x, p);
X_poly = bsxfun(@minus, X_poly, mu);
X_poly = bsxfun(@rdivide, X_poly, sigma);
 
% Add ones
X_poly = [ones(size(x, 1), 1) X_poly];
 
% Plot
plot(x, X_poly * theta, '--', 'LineWidth', 2)
 
% Hold off to the current figure
hold off
end



